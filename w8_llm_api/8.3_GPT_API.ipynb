{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8bbd15",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Working-with-the-GPT-API\" data-toc-modified-id=\"Working-with-the-GPT-API-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Working with the GPT API</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-an-API-KEY\" data-toc-modified-id=\"Getting-an-API-KEY-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Getting an API KEY</a></span></li><li><span><a href=\"#OpenAI-Python-Package-Installation\" data-toc-modified-id=\"OpenAI-Python-Package-Installation-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>OpenAI Python Package Installation</a></span></li><li><span><a href=\"#Chat-Completions-API\" data-toc-modified-id=\"Chat-Completions-API-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Chat Completions API</a></span><ul class=\"toc-item\"><li><span><a href=\"#Chat-completions-response-format\" data-toc-modified-id=\"Chat-completions-response-format-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Chat completions response format</a></span></li><li><span><a href=\"#Conversation-history\" data-toc-modified-id=\"Conversation-history-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Conversation history</a></span></li><li><span><a href=\"#Creating-a-basic-conversation-loop\" data-toc-modified-id=\"Creating-a-basic-conversation-loop-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Creating a basic conversation loop</a></span></li><li><span><a href=\"#Request-parameters\" data-toc-modified-id=\"Request-parameters-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Request parameters</a></span></li></ul></li><li><span><a href=\"#Using-Chat-Completion-for-non-chat-scenarios\" data-toc-modified-id=\"Using-Chat-Completion-for-non-chat-scenarios-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Using Chat Completion for non-chat scenarios</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentiment-Analysis\" data-toc-modified-id=\"Sentiment-Analysis-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Sentiment Analysis</a></span></li><li><span><a href=\"#Language-Translation\" data-toc-modified-id=\"Language-Translation-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Language Translation</a></span></li></ul></li></ul></li><li><span><a href=\"#Extra:-creating-a-chatbot-with-gradio-for-front-end-UI\" data-toc-modified-id=\"Extra:-creating-a-chatbot-with-gradio-for-front-end-UI-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Extra: creating a chatbot with gradio for front-end UI</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c60b66-7b38-4e30-a7f0-dc37873d8f13",
   "metadata": {
    "id": "48c60b66-7b38-4e30-a7f0-dc37873d8f13"
   },
   "source": [
    "# Working with the GPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429d764-4d64-46f7-acd0-8d627d461db4",
   "metadata": {
    "id": "7429d764-4d64-46f7-acd0-8d627d461db4"
   },
   "source": [
    "## Getting an API KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4bc63a-e1d2-47ff-9e1b-4eddbdcf1976",
   "metadata": {
    "id": "5b4bc63a-e1d2-47ff-9e1b-4eddbdcf1976"
   },
   "source": [
    "The API empowers you with greater control and versatility to work with the GPT model (model inside ChatGPT). It also allows seamless integration with other applications.\n",
    "\n",
    "To access the model through the API, you will need an API key. For this demo, you'll be using your LT's API KEY.\n",
    "\n",
    "*To obtain your own API key, you'll need to create an account and set up billing. You can create your account at https://platform.openai.com/.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb399d5-1876-41e5-9116-af97170e786f",
   "metadata": {
    "id": "9cb399d5-1876-41e5-9116-af97170e786f"
   },
   "source": [
    "In this demo, we will read the api key from a txt file. Create a `key.txt` and paste the key your lead teacher gave you in that file.\n",
    "\n",
    "Alternatively, save it as an environment variable and read it as it follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "040e7cbb-d253-46c0-a9da-c59fd9209fd0",
   "metadata": {
    "id": "040e7cbb-d253-46c0-a9da-c59fd9209fd0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705056f-6395-4983-85fc-a56e06a1caff",
   "metadata": {
    "id": "2705056f-6395-4983-85fc-a56e06a1caff"
   },
   "source": [
    "## OpenAI Python Package Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac44c1-7500-4f77-b5b4-0a8f467c669d",
   "metadata": {
    "id": "3fac44c1-7500-4f77-b5b4-0a8f467c669d"
   },
   "source": [
    "To utilize the GPT API, you'll need to have the OpenAI Python package installed.\n",
    "\n",
    "You can easily install it by running the command pip install --upgrade openai. *Adding the --upgrade flag ensures that you have the most up-to-date version, in case you installed openai before, as the GPT API is a recently introduced feature.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae581f1d-c234-4afb-9040-5545661cae33",
   "metadata": {
    "id": "ae581f1d-c234-4afb-9040-5545661cae33"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73da737a-e8a0-4784-8493-c5a10c1ba7e6",
   "metadata": {
    "id": "73da737a-e8a0-4784-8493-c5a10c1ba7e6"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from config import api_key\n",
    "\n",
    "# load and set our key\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af63c0-b790-4170-99b4-d99f29721a7c",
   "metadata": {
    "id": "45af63c0-b790-4170-99b4-d99f29721a7c"
   },
   "source": [
    "## Chat Completions API\n",
    "\n",
    "To use a GPT model via the OpenAI API, you’ll send a request containing the inputs and your API key, and receive a response containing the model’s output.\n",
    "\n",
    "As of July 2023 there are two main APIs endpoints to work with GPT models.\n",
    "- Completions API endpoint: only for the older legacy models\n",
    "- Chat Completions API endpoint: to access the latest models, gpt-4 and gpt-3.5-turbo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c3462-715c-47b0-9ff4-9443958f16ac",
   "metadata": {
    "id": "2f4c3462-715c-47b0-9ff4-9443958f16ac"
   },
   "source": [
    "Chat models in Chat Completions API take as mandatory parameters:\n",
    "- **List of messages as input**\n",
    "- **Model**: we will use gpt-3.5-turbo\n",
    "\n",
    "They return a **model-generated message as output**.\n",
    "\n",
    "An example API call looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d974ced-8f75-43e4-b406-93130aec7c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55346a8c-f16e-4b31-85f5-988e850b9d97",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a famous chef. Share your best cooking tips and tricks.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat are the ingredients for making pancakes?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:667\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1213\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1201\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1210\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1211\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1212\u001b[0m     )\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:902\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    895\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    900\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    901\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:978\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    977\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1026\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:978\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    977\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1026\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    990\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    992\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    996\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    997\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1001\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a famous chef. Share your best cooking tips and tricks.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"What are the ingredients for making pancakes?\"}\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dacaa46-141f-488b-a062-cdc68e853fb4",
   "metadata": {
    "id": "7dacaa46-141f-488b-a062-cdc68e853fb4"
   },
   "source": [
    "In conversations using the Chat Completion API, each message has a **role (\"system,\" \"user,\" or \"assistant\").** Typically, a conversation starts with a system message to set the assistant's behavior, followed by alternating user and assistant messages.\n",
    "- The system message is optional and can be used to customize the assistant's personality or provide specific instructions\n",
    "- User messages contain requests or comments (prompts)\n",
    "- Assistant messages store previous assistant responses or serve as examples of desired behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9de3c0-f9cf-488f-83e2-2dfa96a92b10",
   "metadata": {
    "id": "fe9de3c0-f9cf-488f-83e2-2dfa96a92b10"
   },
   "source": [
    "### Chat completions response format\n",
    "\n",
    "An example chat completions API response looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c1d1665-3e06-45a7-9035-9929b93d3d34",
   "metadata": {
    "id": "3c1d1665-3e06-45a7-9035-9929b93d3d34"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefb398-23c9-42e7-aed3-e23b96df0a7c",
   "metadata": {
    "id": "bbefb398-23c9-42e7-aed3-e23b96df0a7c"
   },
   "source": [
    "In Python, the assistant’s reply can be extracted with response['choices'][0]['message']['content']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7477c92c-8761-4aac-8a5a-c44db8515079",
   "metadata": {
    "id": "7477c92c-8761-4aac-8a5a-c44db8515079"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19981b-1908-4184-aab9-fc77a369e143",
   "metadata": {
    "id": "4a19981b-1908-4184-aab9-fc77a369e143"
   },
   "source": [
    "\n",
    "\n",
    "Every response includes a finish_reason. The possible values for finish_reason are:\n",
    "\n",
    "- stop: API returned complete model output.\n",
    "- length: Incomplete model output due to max_tokens parameter or token limit.\n",
    "- content_filter: Omitted content due to a flag from our content filters.\n",
    "- null: API response still in progress or incomplete.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d21189-e888-4f55-b42d-cf7e7b6a2547",
   "metadata": {
    "id": "d3d21189-e888-4f55-b42d-cf7e7b6a2547"
   },
   "source": [
    "### Conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e62b68-2c6d-4825-92dc-4cf79fdcbb26",
   "metadata": {
    "id": "72e62b68-2c6d-4825-92dc-4cf79fdcbb26"
   },
   "source": [
    "Since it recommended Baking Powder, let's ask how much in another prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6ffe1-be5d-4217-b1b5-a5c25029db8a",
   "metadata": {
    "id": "3fa6ffe1-be5d-4217-b1b5-a5c25029db8a"
   },
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"How much Baking Powder do you recommend for this recipe?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bf4ca-6025-45a6-96d4-ec14b59dfb3d",
   "metadata": {
    "id": "dd4bf4ca-6025-45a6-96d4-ec14b59dfb3d"
   },
   "outputs": [],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c5f3d8-11d0-49d8-88fd-5b47b2789f9d",
   "metadata": {
    "id": "a4c5f3d8-11d0-49d8-88fd-5b47b2789f9d"
   },
   "source": [
    "We can see that the history was not saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85b63e-475b-4330-8c8e-7791074e7dbb",
   "metadata": {
    "id": "8e85b63e-475b-4330-8c8e-7791074e7dbb"
   },
   "source": [
    "**Including conversation history is crucial** when user instructions refer to prior messages. We can do that by sending all the prompts, with its role, in the *messages* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a8d4d-921e-4331-89a8-3bda3576af06",
   "metadata": {
    "id": "d95a8d4d-921e-4331-89a8-3bda3576af06"
   },
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a famous chef. Share your best cooking tips and tricks.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the ingredients for making pancakes?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"To make pancakes, you'll need flour, eggs, milk, baking powder, and a pinch of salt.\"}, # we shortened it for our demo purpose\n",
    "    {\"role\": \"user\", \"content\": \"Can I use almond milk instead of regular milk?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108988dd-38a2-428b-a0e3-9d18613da8f9",
   "metadata": {
    "id": "108988dd-38a2-428b-a0e3-9d18613da8f9"
   },
   "outputs": [],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f3508d-c5e3-4475-b32f-7886984beb60",
   "metadata": {
    "id": "41f3508d-c5e3-4475-b32f-7886984beb60"
   },
   "source": [
    "**Each user instruction relies on the prior messages in the conversation history to make sense.**\n",
    "\n",
    "Since the language models don't have inherent memory of past requests, it's important to include the relevant conversation history in each API request. If the conversation exceeds the model's token limit, it may need to be truncated or shortened while ensuring the essential context and instructions are retained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a446a56-43b9-47de-9e1d-d69ea9051219",
   "metadata": {
    "id": "5a446a56-43b9-47de-9e1d-d69ea9051219"
   },
   "source": [
    "### Creating a basic conversation loop\n",
    "\n",
    "This example demonstrates a conversation loop that performs the following tasks:\n",
    "\n",
    "1. Takes console input continuously and formats it as the user's role content within the messages array.\n",
    "2. Prints the model's response to the console and formats it as the assistant's role content within the messages array.\n",
    "\n",
    "This approach ensures that each time a new question is asked, the ongoing conversation transcript is sent along with the latest question. Since the model lacks memory, it's crucial to include an updated transcript with each new question. Otherwise, the model may lose context from previous questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90675cd-823b-4c68-8bac-6d0149a72297",
   "metadata": {
    "id": "d90675cd-823b-4c68-8bac-6d0149a72297"
   },
   "outputs": [],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "def gpt_response(inp, message_history):\n",
    "    # We save the user's input\n",
    "    message_history.append({\"role\": \"user\", \"content\": f\"{inp}\"})\n",
    "\n",
    "    # Generate a response from the chatbot model\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=message_history\n",
    "    )\n",
    "\n",
    "    # We save the assistant response\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{completion.choices[0].message.content}\"})\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfa9fb-ce9a-4545-bea6-9a479a8edf84",
   "metadata": {
    "id": "67bfa9fb-ce9a-4545-bea6-9a479a8edf84"
   },
   "outputs": [],
   "source": [
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"), message_history) # Lets ask by input different prompts\n",
    "    print(message_history[-1][\"content\"]) # Print the last response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e41f0-c2c2-4ac7-ab3b-7adccb7b7a50",
   "metadata": {
    "id": "5e6e41f0-c2c2-4ac7-ab3b-7adccb7b7a50"
   },
   "source": [
    "Let's check the message history to see if it has all the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee52100-283d-4745-b618-c24ac36a658d",
   "metadata": {
    "id": "3ee52100-283d-4745-b618-c24ac36a658d"
   },
   "outputs": [],
   "source": [
    "message_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d87d2a-f898-4864-8f89-74463bbccf5b",
   "metadata": {
    "id": "f0d87d2a-f898-4864-8f89-74463bbccf5b"
   },
   "source": [
    "### Request parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e409d-2c31-4fed-bded-05573da8343e",
   "metadata": {
    "id": "3e1e409d-2c31-4fed-bded-05573da8343e"
   },
   "source": [
    "Let's take a look at some request parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea9c50f-7852-454a-ad30-69a7c23a2a48",
   "metadata": {
    "id": "eea9c50f-7852-454a-ad30-69a7c23a2a48"
   },
   "source": [
    "- **Model**: Model type (e.g., GPT-3.5 turbo., GPT 4)\n",
    "- **Prompt**: expects a list of messages in a chat-based format\n",
    "- **Temperature (default 1)**: sampling temperature. Between 0 and 2. Higher value means more diverse and random output, while a lower value makes it more focused and deterministic.\n",
    "- **Max tokens (default 16)**: limits the length of the generated response (max length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc0e101-8dbc-4673-b3fd-b012ad16eb0d",
   "metadata": {
    "id": "7cc0e101-8dbc-4673-b3fd-b012ad16eb0d"
   },
   "outputs": [],
   "source": [
    "# Lets rewrite the gpt_response function to include possible parameters\n",
    "\n",
    "def gpt_response(inp, message_history, **params):\n",
    "    # We save the user's input\n",
    "    message_history.append({\"role\": \"user\", \"content\": f\"{inp}\"})\n",
    "\n",
    "    # Generate a response from the chatbot model\n",
    "    completion_params = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": message_history,\n",
    "        **params  # Include additional parameters\n",
    "    }\n",
    "\n",
    "    completion = openai.ChatCompletion.create(**completion_params) # Include additional parameters\n",
    "\n",
    "    # We save the assistant response\n",
    "    message_history.append({\"role\": \"assistant\", \"content\": f\"{completion.choices[0].message.content}\"})\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f677e-d938-4a93-8395-5a0476c3f823",
   "metadata": {
    "id": "184f677e-d938-4a93-8395-5a0476c3f823"
   },
   "source": [
    "Lets explore different settings by using a max_tokens value of 100 and testing three temperature levels (0, 1, and 2) to generate responses from the model, completing the prompt \"My favourite animal is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e662eb-2ec3-4620-a8a7-3a10b1e2358e",
   "metadata": {
    "id": "61e662eb-2ec3-4620-a8a7-3a10b1e2358e"
   },
   "outputs": [],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"Complete the prompt.\"}]\n",
    "\n",
    "for i in [0,1,2]:\n",
    "    message_history = gpt_response(\"My favourite animal is \",\n",
    "                                   message_history,\n",
    "                                   max_tokens=100,\n",
    "                                   temperature=i)\n",
    "    print(message_history[-1][\"content\"]+\"\\n\") # Print the last response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a199af8-3690-4dd2-b6d3-9eeb3e852608",
   "metadata": {
    "id": "8a199af8-3690-4dd2-b6d3-9eeb3e852608"
   },
   "source": [
    "- **top_p** (Defaults to 1)\n",
    "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "\n",
    "A higher value gives access to more tokens (and more diversity) and a lower value is more deterministic.\n",
    "\n",
    "We generally recommend altering this or temperature but not both.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c39d3-9d50-4cc0-b39d-26410cfa3a51",
   "metadata": {
    "id": "b65c39d3-9d50-4cc0-b39d-26410cfa3a51"
   },
   "source": [
    "Lets explore different settings by using a max_tokens value of 100 and testing two top_p levels (0 and 1) to generate responses from the model, completing the prompt \"My favourite animal is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3323f-f839-45f9-a653-66d7be6af63b",
   "metadata": {
    "id": "66e3323f-f839-45f9-a653-66d7be6af63b"
   },
   "outputs": [],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"Complete the prompt.\"}]\n",
    "\n",
    "for i in [0,1]:\n",
    "    message_history = gpt_response(\"My favourite animal is \",\n",
    "                                   message_history,\n",
    "                                   max_tokens=100,\n",
    "                                   top_p=i)\n",
    "    print(message_history[-1][\"content\"]+\"\\n\") # Print the last response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb50d3-2cc5-42b6-9af9-8101e826e349",
   "metadata": {
    "id": "cdcb50d3-2cc5-42b6-9af9-8101e826e349"
   },
   "source": [
    "- **presence_penalty** (Defaults to 0)\n",
    "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. **Higher values promote creativity by penalising the model when it uses predefined tokens.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3416cd-17f9-4be7-8098-b1fdb8177ede",
   "metadata": {
    "id": "0c3416cd-17f9-4be7-8098-b1fdb8177ede"
   },
   "source": [
    "- **frequency_penalty** (Defaults to 0)\n",
    "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. **Higher values penalise the model for repetition and reward variety.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c356ccf-0eb6-4ff7-afd6-9b1d8f958b0c",
   "metadata": {
    "id": "9c356ccf-0eb6-4ff7-afd6-9b1d8f958b0c"
   },
   "source": [
    "Lets explore different settings by using a max_tokens value of 100 and testing two presence penalty and frequency penalty levels (-2 and 2) to generate responses from the model, using the prompt \"generate 20 ways to say you can't buy that because you're broke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17a621-13b1-4afa-a528-3e70fd5071e7",
   "metadata": {
    "id": "5e17a621-13b1-4afa-a528-3e70fd5071e7"
   },
   "outputs": [],
   "source": [
    "message_history=[]\n",
    "\n",
    "for i in [-2,2]:\n",
    "    message_history = gpt_response(\"generate 20 ways to say you can't buy that because you're broke\",\n",
    "                                   message_history,\n",
    "                                   max_tokens=100,\n",
    "                                   presence_penalty=i,\n",
    "                                   frequency_penalty=i)\n",
    "    print(message_history[-1][\"content\"]+\"\\n\")# Print the last response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413709d5-199a-4afb-81e7-02486a7bf084",
   "metadata": {
    "id": "413709d5-199a-4afb-81e7-02486a7bf084"
   },
   "source": [
    "## Using Chat Completion for non-chat scenarios\n",
    "The Chat Completion API is designed to work with multi-turn conversations, but it also works well for non-chat scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6ecec-2568-4c64-ba66-31ade5729634",
   "metadata": {
    "id": "a2c6ecec-2568-4c64-ba66-31ade5729634"
   },
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f9fff-5608-400c-9222-d2f4e1ae8db1",
   "metadata": {
    "id": "843f9fff-5608-400c-9222-d2f4e1ae8db1"
   },
   "source": [
    "Lets set up a sentiment analysis scenario where a user inputs a tweet, and the program generates responses using GPT, providing sentiment predictions until interrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb85b49-1fa6-42cb-a770-1494dcfd0566",
   "metadata": {
    "id": "beb85b49-1fa6-42cb-a770-1494dcfd0566"
   },
   "source": [
    "We will give the role *system* the task to decide whether a Tweet's sentiment is positive, neutral, or negative.\n",
    "We include as a user pre-prompt the example \"I loved the new Batman movie! Sentiment:\", and an example assistant answer \"Positive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef458f-b55b-4d15-b5ce-7d26e40761af",
   "metadata": {
    "id": "d7ef458f-b55b-4d15-b5ce-7d26e40761af"
   },
   "outputs": [],
   "source": [
    "# Lets give an example of sentiment analysis\n",
    "message_history=[{\"role\": \"system\", \"content\": \"Decide whether a Tweet's sentiment is positive, neutral, or negative.\"},\n",
    "             {\"role\": \"user\", \"content\": \"Tweet: \\\"I loved the new Batman movie!\\\"\\nSentiment:\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"Positive\"}\n",
    "  ]\n",
    "print(\"Write a tweet: \")\n",
    "\n",
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"),\n",
    "                                   message_history,\n",
    "                                   temperature=0,\n",
    "                                   max_tokens=60,\n",
    "                                   frequency_penalty=0.5)\n",
    "    print(message_history[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5fdca-17c4-4910-9e68-54a5922c5fc1",
   "metadata": {
    "id": "75b5fdca-17c4-4910-9e68-54a5922c5fc1"
   },
   "source": [
    "### Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d297f56-e4ae-48c5-a5bc-7b9e3470881f",
   "metadata": {
    "id": "4d297f56-e4ae-48c5-a5bc-7b9e3470881f"
   },
   "source": [
    "Lets set up a translation scenario where a user inputs a phrase to be translated into Spanish and Portuguese. The program generates responses using GPT, providing translations until interrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1b764-5cf8-4e5b-8fe4-35a62c92313f",
   "metadata": {
    "id": "92d1b764-5cf8-4e5b-8fe4-35a62c92313f"
   },
   "source": [
    "We will give as a system pre-prompt *Translate this into Spanish, Portuguese, Italian*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0fa03-7b43-446d-8118-2f49facb2fc8",
   "metadata": {
    "id": "26a0fa03-7b43-446d-8118-2f49facb2fc8"
   },
   "outputs": [],
   "source": [
    "message_history=[{\"role\": \"system\", \"content\": \"Translate this into Spanish, Portuguese, Italian\"}\n",
    "  ]\n",
    "print(\"Write a phrase to translate to Spanish, Portuguese and Italian: \")\n",
    "\n",
    "while(True):\n",
    "    message_history = gpt_response(input(\"> \"), message_history,temperature=0.3,max_tokens=60)\n",
    "    print(message_history[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b193c4-956a-42aa-b00b-23ddb07865bb",
   "metadata": {
    "id": "44b193c4-956a-42aa-b00b-23ddb07865bb"
   },
   "source": [
    "# Extra: creating a chatbot with gradio for front-end UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ae714-b13a-42a3-a13a-6213ed7ff1b4",
   "metadata": {
    "id": "710ae714-b13a-42a3-a13a-6213ed7ff1b4"
   },
   "source": [
    "Gradio is a Python library that allows you to quickly create customizable UIs for machine learning models, or for any kind of Python function or code snippet, with just a few lines of code. It simplifies the process of deploying and sharing models or code by generating interactive interfaces for input and output data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14a415-9fe0-49f6-8693-4576d2c74ac7",
   "metadata": {
    "id": "1d14a415-9fe0-49f6-8693-4576d2c74ac7"
   },
   "source": [
    "To create a chatbot with Gradio for the front-end user interface, follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f1e1a-26f4-4b96-8fef-51436880d4a4",
   "metadata": {
    "id": "ca8f1e1a-26f4-4b96-8fef-51436880d4a4"
   },
   "source": [
    "1. Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de7b14-5c2f-4456-8ef8-6907d2d96e7b",
   "metadata": {
    "id": "87de7b14-5c2f-4456-8ef8-6907d2d96e7b"
   },
   "outputs": [],
   "source": [
    "#!pip install gradio openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93819469-c55a-488f-b4fa-6d5e21e597a8",
   "metadata": {
    "id": "93819469-c55a-488f-b4fa-6d5e21e597a8"
   },
   "source": [
    "2. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49384515-5e93-4f89-acbb-884154a8768a",
   "metadata": {
    "id": "49384515-5e93-4f89-acbb-884154a8768a"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "# import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d1c73-f986-494e-96b7-77aa4ba421bf",
   "metadata": {
    "id": "b39d1c73-f986-494e-96b7-77aa4ba421bf"
   },
   "source": [
    "3. Set up your OpenAI API credentials by assigning your API key to openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7b881-19b9-478c-8c5b-5bcd85c52a80",
   "metadata": {
    "id": "c1d7b881-19b9-478c-8c5b-5bcd85c52a80"
   },
   "outputs": [],
   "source": [
    "# openai.api_key = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3520acd-ed0e-47ee-b213-9b45142774e6",
   "metadata": {
    "id": "a3520acd-ed0e-47ee-b213-9b45142774e6"
   },
   "source": [
    "4. Define a function that generates chatbot responses based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0385d-822a-42ab-8f8d-067ae2cc09c7",
   "metadata": {
    "id": "ffd0385d-822a-42ab-8f8d-067ae2cc09c7"
   },
   "outputs": [],
   "source": [
    "# Set up the conversation history with user and assistant messages\n",
    "message_history = [{\"role\": \"system\", \"content\": f\"Respond my prompts in a Harry Potter style\"},\n",
    "                   {\"role\": \"assistant\", \"content\": f\"OK\"}]\n",
    "\n",
    "def gpt_response_ui(inp):\n",
    "    global message_history\n",
    "\n",
    "    message_history = gpt_response(inp, message_history, max_tokens = 50)\n",
    "\n",
    "    # Get pairs (as tuples) of msg[\"content\"] from message history,representing one exchange between the user and the chatbot, skipping the pre-prompt\n",
    "    response = [(message_history[i][\"content\"], message_history[i+1][\"content\"]) for i in range(2, len(message_history)-1, 2)]  # convert to tuples of list\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42101a7-55bc-4052-9a5a-368d2472579d",
   "metadata": {
    "id": "c42101a7-55bc-4052-9a5a-368d2472579d"
   },
   "source": [
    "5. Define the Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060004ad-f547-48a4-8d0e-b52ca38d957a",
   "metadata": {
    "id": "060004ad-f547-48a4-8d0e-b52ca38d957a"
   },
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo: #creates a Gradio interface\n",
    "\n",
    "    chatbot = gr.Chatbot() #creates a chatbot instance\n",
    "\n",
    "    with gr.Row(): #creates a row within the Gradio interface to contain components\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\") #text input field\n",
    "        # `show_label=False` parameter hides the label associated with the textbox\n",
    "\n",
    "    txt.submit(gpt_response_ui, txt, chatbot) #sets the submit action to the `gpt_response` function\n",
    "    txt.submit(lambda :\"\", None, txt) #this clears the textbox when the user submits their input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68df5c2-c3e2-41a8-9b7b-ca6fdf4d600b",
   "metadata": {
    "id": "a68df5c2-c3e2-41a8-9b7b-ca6fdf4d600b"
   },
   "source": [
    "6. Run the Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398916f-bb1e-4eda-b9f2-73ceba2695e3",
   "metadata": {
    "id": "3398916f-bb1e-4eda-b9f2-73ceba2695e3"
   },
   "outputs": [],
   "source": [
    "demo.launch(share=True) #To create a public link, set `share=True` in `launch()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03d0ae4-659d-4324-a253-b31c54fdc97f",
   "metadata": {
    "id": "b03d0ae4-659d-4324-a253-b31c54fdc97f"
   },
   "source": [
    "Source Inspiration: MIT License - Copyright (c) 2023 Harrison"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
