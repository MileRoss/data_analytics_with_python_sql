{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787de3d5-0d40-4a23-90ff-1e5279eaea4f",
   "metadata": {},
   "source": [
    "# table of contents\n",
    "1. [predictions, part I](#predictions,-part-I)\n",
    "2. [preprocessing](#preprocessing)\n",
    "3. [calculations](#calculations)\n",
    "   1. [batch model evaluation](#batch-model-evaluation)\n",
    "   2. [single model evaluation](#single-model-evaluation)\n",
    "   3. [LogisticRegression, coefficients](#LogisticRegression,-coefficients)\n",
    "   4. [DecisionTreeClassifier, tree importance](#DecisionTreeClassifier,-tree-importance) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9096ef94-048d-4a5e-8d6b-8e7cf4bc87b5",
   "metadata": {},
   "source": [
    "# predictions, part I\n",
    "- drop columns: no\n",
    "- `scaling: yes`\n",
    "- hyperparameter tuning: no\n",
    "- one-hot encoding: yes, the dataset was received encoded\n",
    "- resampling: no\n",
    "\n",
    "`The main takeaways from this session:`\n",
    "- I'm testing every model's performance on unscaled and scaled data.\n",
    "- RandomForestClassifier is the best performer, as a standalone model, and a base estimator to 2 ensemble methods.\n",
    "- Scaling only significantly improves accuracy for KNN and Bagging with KNN as base estimator.\n",
    "- AdaBoostClassifier has an extremely high computational cost when paired with certain 2 base estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a40560-ee3a-45be-b497-bc77db6d5392",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f7b3b-af0a-4b48-b4ac-27a7d5feef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%run common_imports.py\n",
    "\n",
    "# load and split data\n",
    "%run load_and_split_data.py\n",
    "X_train, X_test, y_train, y_test = load_and_split_data()\n",
    "\n",
    "# scale data\n",
    "%run minmaxscaler.py\n",
    "X_train_scaled, X_test_scaled = scale_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db2424-6d1b-4c36-893f-ac400f8dbd55",
   "metadata": {},
   "source": [
    "# calculations\n",
    "\n",
    "There are 2 approaches, depending on the needs and computing capacities:\n",
    "\n",
    "\n",
    "1. `batch:` calculates accuracy scores and computation time for a group of models, saves their accuracy scores and runtime into a dataframe.\\\n",
    "This approach is recommended because the results can be easily collected and compared compared in the next iterations: unscaled vs scaled data, etc.\\\n",
    "Caution: computation time for this dataset is around 20 minutes.\n",
    "\n",
    "2. `single model:` calculates accuracy score, classification report, and runtime for a single model; doesn't save the results into a dataframe.\n",
    "\n",
    "Both approaches require the timer function, defined in the cell below. For reusability, the function is saved in the utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2f1f4e-07e5-4c2e-b044-56f7525bfe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a decorator to measure the runtime of functions\n",
    "\n",
    "from time import time\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, `kwargs):\n",
    "        start_time = time()\n",
    "        result = func(*args, `kwargs)\n",
    "        end_time = time()\n",
    "        runtime = int(end_time - start_time)\n",
    "        return result, runtime\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3628775-6911-43b3-a60f-ce5cf07bcc46",
   "metadata": {},
   "source": [
    "## batch model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0120fc-e90b-4f4e-b149-05817b0709cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy score and computation cost for a group of models, save the results into a DataFrame\n",
    "\n",
    "# List of models\n",
    "models = [\n",
    "    KNeighborsClassifier(),\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    BaggingClassifier(KNeighborsClassifier()),\n",
    "    BaggingClassifier(LogisticRegression()),\n",
    "    BaggingClassifier(RandomForestClassifier()),\n",
    "    BaggingClassifier(GradientBoostingClassifier()),\n",
    "    AdaBoostClassifier(),\n",
    "    AdaBoostClassifier(LogisticRegression()),\n",
    "    AdaBoostClassifier(RandomForestClassifier()),\n",
    "    AdaBoostClassifier(GradientBoostingClassifier())\n",
    "]\n",
    "\n",
    "# Empty lists to store results\n",
    "model_names = []\n",
    "estimator_names = []\n",
    "accuracies = []\n",
    "times = []\n",
    "sources = []\n",
    "\n",
    "# Train and evaluate model, returning accuracy and runtime\n",
    "@timer\n",
    "def train_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    display(model.fit(X_train, y_train))\n",
    "    accuracy = round(model.score(X_test, y_test) * 100, 2)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy and time for each model with both scaled and unscaled data\n",
    "for model in models:\n",
    "    for scaled in [False, True]:\n",
    "        X_train_use = X_train_scaled if scaled else X_train\n",
    "        X_test_use = X_test_scaled if scaled else X_test\n",
    "\n",
    "        model_name = str(model).split(\"(\")[0]\n",
    "        if hasattr(model, 'base_estimator_'):\n",
    "            estimator_name = str(model.base_estimator_).split('(')[0]\n",
    "        elif hasattr(model, 'estimator'):\n",
    "            estimator_name = str(model.estimator).split('(')[0]\n",
    "        else:\n",
    "            estimator_name = model_name\n",
    "\n",
    "        accuracy, time_taken = train_evaluate_model(model, X_train_use, y_train, X_test_use, y_test)\n",
    "        \n",
    "        model_names.append(model_name)\n",
    "        estimator_names.append(estimator_name)\n",
    "        accuracies.append(accuracy)\n",
    "        times.append(time_taken)\n",
    "        sources.append(\"scaled\" if scaled else \"unscaled\")\n",
    "\n",
    "# Create DataFrame\n",
    "accuracies_without_parameters = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"estimator\": estimator_names,\n",
    "    \"accuracy_in_%\": accuracies,\n",
    "    \"runtime_in_seconds\": times,\n",
    "    \"source\": sources\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "accuracies_without_parameters.to_csv(\"../data/test.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "accuracies_without_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f165dfe-43ae-4000-8d0a-29809d1e20e1",
   "metadata": {},
   "source": [
    "## single model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a50034-83c6-47a4-ad82-cb664a7212e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate a single model, returning accuracy, predictions, and runtime\n",
    "\n",
    "@timer\n",
    "def train_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = round(model.score(X_test, y_test) * 100, 2)\n",
    "    pred = model.predict(X_test)\n",
    "    return accuracy, pred\n",
    "\n",
    "# Wrapper function to handle model instantiation and evaluation\n",
    "def train_evaluate_runtime(model_class, X_train, y_train, X_test, y_test, *args, `kwargs):\n",
    "    model = model_class(*args, `kwargs)\n",
    "    (result, runtime) = train_evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "    accuracy, pred = result  \n",
    "\n",
    "    print(f\"The accuracy of the model is {accuracy}%\")\n",
    "    print(f\"Runtime (seconds): {runtime}\\n\")\n",
    "    print(classification_report(y_true=y_test, y_pred=pred))\n",
    "    \n",
    "    return model_class.__name__, accuracy, runtime, pred\n",
    "\n",
    "# Example usage:\n",
    "# model_name, accuracy, runtime, predictions = train_evaluate_runtime(KNeighborsClassifier, X_train, y_train, X_test, y_test, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569541d3-03c8-410b-b238-74771ea8105b",
   "metadata": {},
   "source": [
    "## LogisticRegression, coefficients\n",
    "Below is #log_reg_coefficients function that calculates and displays the coefficients of the features in a logistic regression model, sorted by their absolute values in descending order, indicating their importance or impact on the model's predictions.\n",
    "\n",
    "`Top 5 features by absolute coefficient:`\n",
    "1. deposit_type_No_Deposit 1.97\n",
    "2. deposit_type_Non_Refund 1.69\n",
    "3. previous_cancellations 1.27\n",
    "4. required_car_parking_spaces 0.98\n",
    "5. market_segment_Offline_TA_TO 0.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a044931-6717-495c-a2f0-f96b23d23ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_coefficients():\n",
    "    coefficients = LogisticRegression().fit(X_train, y_train).coef_[0]\n",
    "    feature_names = X_train.columns\n",
    "    coefficients_df = pd.DataFrame({\"Feature\": feature_names, \"Coefficient\": coefficients})\n",
    "    coefficients_df[\"Absolute Coefficient\"] = abs(coefficients_df[\"Coefficient\"])\n",
    "    coefficients_df[[\"Coefficient\", \"Absolute Coefficient\"]] = coefficients_df[[\"Coefficient\", \"Absolute Coefficient\"]].round(2)\n",
    "    coefficients_df = coefficients_df.sort_values(by=\"Absolute Coefficient\", ascending=False)\n",
    "    display(coefficients_df)\n",
    "\n",
    "log_reg_coefficients()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a779ac45-a8bc-484b-8156-3d75b3782356",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier, tree importance\n",
    "Below is #dt_tree_importance function that calculates importance of each feature in the decision tree, sorts them in descending order of importance, displays as a DataFrame, generates+displays+saves a visualisation of the tree.\n",
    "\n",
    "With the max_depth=2 parameter, this tree returns Top 3 features in the decision tree (and their importance score):\n",
    "1. `deposit_type_Non_Refund (0.88):` there are more refundable canceled bookings in absolute terms, but 99% of non-refundable ones were canceled.\n",
    "2. lead_time (0.12): refundable bookings with lead times <= 14.5 days are less likely to be canceled.\n",
    "3. previous_bookings_not_canceled (almost 0): customers with history of not canceling previous bookings is a strong predictor of not canceling the current booking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e77ccb5-9860-4797-9fad-562490038ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_tree_importance():\n",
    "    dt = DecisionTreeClassifier(max_depth=2)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    tree_importance = {feature: f\"{importance:.2f}\" for feature, importance in zip(X_train.columns, dt.feature_importances_)}\n",
    "    sorted_tree_importance = {k: v for k, v in sorted(tree_importance.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    df = pd.DataFrame(sorted_tree_importance.items(), columns=[\"Feature\", \"Importance\"])\n",
    "    display(df)\n",
    "\n",
    "    dot_data = export_graphviz(dt, out_file=None, filled=True, rounded=True, feature_names=X_train.columns)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graph.format = \"png\"\n",
    "    graph.render(\"decision_tree_unscaled\")\n",
    "    display(graph)\n",
    "\n",
    "dt_tree_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b1c732-712d-4947-b43b-d6599569ed82",
   "metadata": {},
   "source": [
    "Next: notebook_05_machine_learning_02_hyperparameter_tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
